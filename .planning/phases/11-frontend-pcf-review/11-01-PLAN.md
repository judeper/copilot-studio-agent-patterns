---
phase: 11-frontend-pcf-review
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/11-frontend-pcf-review/11-01-correctness-findings.md
  - .planning/phases/11-frontend-pcf-review/11-01-implementability-findings.md
  - .planning/phases/11-frontend-pcf-review/11-01-gaps-findings.md
autonomous: true
requirements:
  - PCF-01
  - PCF-02
  - PCF-03
  - PCF-04
  - PCF-05

must_haves:
  truths:
    - "Component architecture has been validated for consistent state/props/hooks patterns with no structural anti-patterns"
    - "Every v2.0 tech debt item is categorized as deploy-blocking or deferrable with documented rationale"
    - "Test coverage is assessed for all user-facing components and critical hooks with gaps identified"
    - "Data flow from Dataverse DataSet through useCardData hook through component render has been traced and verified"
    - "Error states, loading states, and edge cases are handled in every user-facing component"
  artifacts:
    - path: ".planning/phases/11-frontend-pcf-review/11-01-correctness-findings.md"
      provides: "Correctness Agent findings — validates types, props, state, hooks, rendering logic are factually correct"
    - path: ".planning/phases/11-frontend-pcf-review/11-01-implementability-findings.md"
      provides: "Implementability Agent findings — validates components work correctly at runtime in a Canvas App PCF context"
    - path: ".planning/phases/11-frontend-pcf-review/11-01-gaps-findings.md"
      provides: "Gaps Agent findings — identifies missing error handling, untested paths, tech debt, and UX gaps"
  key_links:
    - from: "src/AssistantDashboard/hooks/useCardData.ts"
      to: "src/AssistantDashboard/components/App.tsx"
      via: "Hook provides card data consumed by App and distributed to children"
      pattern: "useCardData"
    - from: "src/AssistantDashboard/index.ts"
      to: "src/AssistantDashboard/components/App.tsx"
      via: "PCF lifecycle (init/updateView/destroy) bridges to React rendering"
      pattern: "ReactDOM.render"
    - from: "schemas/output-schema.json"
      to: "src/AssistantDashboard/components/types.ts"
      via: "TypeScript interfaces must match Dataverse column types from schema"
      pattern: "AssistantCard"
---

<objective>
Three independent AI Council agents review all frontend/PCF-layer artifacts in parallel: Correctness (are types, props, state management, hook contracts, and rendering logic factually accurate?), Implementability (do components work correctly at runtime in a Canvas App PCF virtual control context?), and Gaps (what error handling, test coverage, tech debt, or UX gaps exist?).

Purpose: Produce three independent review reports covering every PCF source file from different angles. The AI Council pattern (established in Phase 10) ensures no blind spots — each agent has its own mandate and checklist focused on frontend concerns.

Output: Three findings documents, one per agent perspective, each with categorized issues (deploy-blocking vs. non-blocking), evidence from specific source files, and remediation suggestions.
</objective>

<execution_context>
@/Users/admin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/admin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Phase 10 summary (for AI Council pattern reference):
@.planning/phases/10-platform-architecture-review/10-01-SUMMARY.md

Frontend/PCF source files under review (read ALL of these):
@enterprise-work-assistant/src/AssistantDashboard/components/types.ts
@enterprise-work-assistant/src/AssistantDashboard/components/constants.ts
@enterprise-work-assistant/src/AssistantDashboard/components/App.tsx
@enterprise-work-assistant/src/AssistantDashboard/components/BriefingCard.tsx
@enterprise-work-assistant/src/AssistantDashboard/components/CardDetail.tsx
@enterprise-work-assistant/src/AssistantDashboard/components/CardGallery.tsx
@enterprise-work-assistant/src/AssistantDashboard/components/CardItem.tsx
@enterprise-work-assistant/src/AssistantDashboard/components/CommandBar.tsx
@enterprise-work-assistant/src/AssistantDashboard/components/ConfidenceCalibration.tsx
@enterprise-work-assistant/src/AssistantDashboard/components/FilterBar.tsx
@enterprise-work-assistant/src/AssistantDashboard/hooks/useCardData.ts
@enterprise-work-assistant/src/AssistantDashboard/utils/urlSanitizer.ts
@enterprise-work-assistant/src/AssistantDashboard/index.ts
@enterprise-work-assistant/src/AssistantDashboard/generated/ManifestTypes.d.ts

Test files (read ALL of these):
@enterprise-work-assistant/src/AssistantDashboard/components/__tests__/App.test.tsx
@enterprise-work-assistant/src/AssistantDashboard/components/__tests__/BriefingCard.test.tsx
@enterprise-work-assistant/src/AssistantDashboard/components/__tests__/CardDetail.test.tsx
@enterprise-work-assistant/src/AssistantDashboard/components/__tests__/CardGallery.test.tsx
@enterprise-work-assistant/src/AssistantDashboard/components/__tests__/CardItem.test.tsx
@enterprise-work-assistant/src/AssistantDashboard/components/__tests__/CommandBar.test.tsx
@enterprise-work-assistant/src/AssistantDashboard/components/__tests__/FilterBar.test.tsx
@enterprise-work-assistant/src/AssistantDashboard/hooks/__tests__/useCardData.test.ts
@enterprise-work-assistant/src/AssistantDashboard/utils/__tests__/urlSanitizer.test.ts

Configuration files (read ALL of these):
@enterprise-work-assistant/src/package.json
@enterprise-work-assistant/src/tsconfig.json
@enterprise-work-assistant/src/tsconfig.test.json
@enterprise-work-assistant/src/test/jest.config.ts
@enterprise-work-assistant/src/test/jest.setup.ts
@enterprise-work-assistant/src/.eslintrc.json

Schema files (for cross-reference):
@enterprise-work-assistant/schemas/output-schema.json
@enterprise-work-assistant/schemas/dataverse-table.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Correctness Agent — validate types, state management, hooks, and rendering logic across PCF components</name>
  <files>.planning/phases/11-frontend-pcf-review/11-01-correctness-findings.md</files>
  <action>
Read every frontend/PCF source file, test file, and configuration file listed in the context section above. Also read the two schema files for cross-reference. Act as the **Correctness Agent** — your mandate is to verify that every type definition, prop contract, state management pattern, hook implementation, and rendering path is factually accurate.

**Checklist — work through each systematically:**

1. **Type Definitions** (types.ts, constants.ts):
   - Verify every field in `AssistantCard` interface matches a real Dataverse column from dataverse-table.json (correct name, compatible type)
   - Verify enum types (Priority, TriageTier, TriggerType, TemporalHorizon, DraftType, OutcomeAction) match the Choice option labels in dataverse-table.json
   - Verify nullable fields (Priority | null, TemporalHorizon | null) are correctly identified — which fields are truly optional?
   - Verify PRIORITY_COLORS and any other constant maps use valid Fluent UI v9 token names
   - Cross-check: every field in output-schema.json has a corresponding TypeScript type

2. **Data Hook** (useCardData.ts):
   - Verify the DataSet column mapping (dataset.records → AssistantCard) is correct for every field
   - Verify the N/A-to-null ingestion boundary maps are correct and complete (no N/A strings leaking to UI)
   - Verify type coercion (String(), Number(), Boolean()) is applied correctly per field type
   - Verify date parsing (createdOn, lastModified) handles null/undefined safely
   - Verify the hook returns the correct shape for consumers (App.tsx)
   - Cross-check: does the hook handle all columns declared in types.ts?

3. **PCF Entry Point** (index.ts):
   - Verify the PCF lifecycle methods (init, updateView, getOutputs, destroy) follow the ComponentFramework.StandardControl contract
   - Verify React mounting and unmounting is handled correctly (no memory leaks, proper cleanup)
   - Verify the DataSet binding passes data correctly to the React tree
   - Verify output properties (if any) are set correctly for Canvas App consumption
   - Verify the FluentProvider wrapping is correct (theme, target document)

4. **Component Architecture** (all .tsx files):
   - Verify each component receives the correct props based on its parent's data flow
   - Verify state management patterns are consistent (useState for local state, props for shared state)
   - Verify callback props (onSelect, onDismiss, onFilter, etc.) are typed correctly and invoked properly
   - Verify conditional rendering logic is correct (null checks, empty array checks, loading states)
   - Verify Fluent UI v9 component usage: correct prop names, correct import paths, correct component variants
   - Cross-check component hierarchy: App → CardGallery → CardItem, App → CardDetail, App → FilterBar, App → CommandBar, App → BriefingCard, App → ConfidenceCalibration

5. **Specific Component Correctness**:
   - **App.tsx**: Verify filter logic, view state management, card selection, briefing/command/calibration panel toggling
   - **CardDetail.tsx**: Verify all AssistantCard fields are rendered with correct formatting (dates, enums, links via isSafeUrl)
   - **CardItem.tsx**: Verify summary display, priority badge, tier badge, and click handler
   - **BriefingCard.tsx**: Verify briefing data rendering, schedule configuration, staleness indicators
   - **CommandBar.tsx**: Verify command input, orchestrator response handling, action execution
   - **ConfidenceCalibration.tsx**: Verify tab structure, metric calculations, chart rendering
   - **FilterBar.tsx**: Verify filter state binding and callback invocation
   - **CardGallery.tsx**: Verify card list rendering and selection propagation

6. **URL Sanitizer** (urlSanitizer.ts):
   - Verify isSafeUrl correctly validates URL protocols (https:, mailto: only per Phase 5 decision)
   - Verify edge cases: null/undefined input, malformed URLs, javascript: protocol, data: URIs

Write the findings document with this structure:
```
# Correctness Agent — Frontend/PCF Findings

## Summary
[X issues found: N deploy-blocking, M non-blocking]

## Methodology
[What was checked and against what source of truth]

## Findings

### Deploy-Blocking Issues
[Issues that would cause runtime errors or incorrect behavior in a deployed Canvas App]
For each: ID, artifact, location (file:line), issue description, evidence, suggested fix

### Non-Blocking Issues
[Issues that are technically incorrect but would not prevent deployment]
For each: ID, artifact, location, issue description, evidence, suggested fix

### Validated (No Issues)
[Areas that passed correctness checks — brief list]
```

Be thorough. Cross-reference types.ts against output-schema.json and dataverse-table.json. Trace data flow from useCardData through every component that consumes card data. If something looks correct but you cannot verify it, flag as "unverified."
  </action>
  <verify>
    <automated>test -f .planning/phases/11-frontend-pcf-review/11-01-correctness-findings.md && grep -c "Deploy-Blocking\|Non-Blocking\|Validated" .planning/phases/11-frontend-pcf-review/11-01-correctness-findings.md</automated>
  </verify>
  <done>Correctness findings document exists with categorized issues (deploy-blocking/non-blocking), each backed by specific evidence from source files, type cross-references against schemas, and component hierarchy validation</done>
</task>

<task type="auto">
  <name>Task 2: Implementability Agent — validate components work correctly at runtime in a Canvas App PCF context</name>
  <files>.planning/phases/11-frontend-pcf-review/11-01-implementability-findings.md</files>
  <action>
Read every frontend/PCF source file, test file, and configuration file listed in the context section above. Act as the **Implementability Agent** — your mandate is to verify that every component, hook, and configuration will actually work correctly when deployed as a PCF virtual control inside a Canvas App.

**Checklist — work through each systematically:**

1. **PCF Virtual Control Runtime**:
   - Can the control be loaded in a Canvas App via the standard PCF hosting mechanism?
   - Does index.ts correctly implement the StandardControl interface? Are any lifecycle methods missing or incorrectly implemented?
   - Does the ControlManifest.Input.xml (referenced via ManifestTypes.d.ts) declare the correct data-set binding for Dataverse views?
   - Is React 16.14.0 (platform-provided via `platform-library` manifest element) used correctly? Any React 18+ APIs accidentally used?
   - Does the FluentProvider correctly set up Fluent UI v9 theming in the PCF shadow DOM context?
   - Are there any window/document references that would break in Canvas App's iframe sandbox?

2. **Data Binding and Rendering**:
   - Does the DataSet paging work correctly? (dataset.paging.loadNextPage, hasNextPage)
   - Does the useCardData hook handle empty datasets, loading states, and dataset refresh cycles?
   - What happens when updateView is called with a dataset that has 0 records? 500 records? 2000 records (delegation limit)?
   - Does the component handle Dataverse column types correctly at runtime (Choice columns return option values, DateTime returns ISO strings, etc.)?
   - Are there any fields where the DataSet API returns a different type than what TypeScript expects?

3. **Component Runtime Behavior**:
   - Does App.tsx handle initial render before data loads (loading state)?
   - Do all components handle undefined/null props gracefully (no "cannot read property of undefined" at runtime)?
   - Does CardDetail.tsx handle every combination of nullable fields (some cards have no draft, no temporal horizon, no links)?
   - Does BriefingCard handle the absence of briefing data (no briefing has been generated yet)?
   - Does CommandBar handle orchestrator timeout or failure (network error, agent error)?
   - Does ConfidenceCalibration handle zero-data states (no cards triaged yet, no outcomes recorded)?
   - Does FilterBar re-render correctly when filters are applied/cleared?

4. **Event Handling and State Propagation**:
   - Do click handlers propagate correctly through the component tree? (CardItem click → App state change → CardDetail appears)
   - Do output bindings (if any) correctly send data back to the Canvas App host?
   - Does the command bar's action execution pattern (executing orchestrator commands) work within PCF constraints?
   - Are there any event handlers that could cause infinite re-render loops?
   - Is there any state that should be lifted to a higher component but is trapped in a child?

5. **Build and Deploy Pipeline**:
   - Does `npm run build` (pcf-scripts build) produce a valid control bundle?
   - Does the tsconfig.json configuration produce valid output for the pcf-scripts bundler?
   - Are all imports resolvable at build time? Any circular dependencies?
   - Does the package.json postinstall script (ManifestSchema.json patching) work reliably?
   - Are there any build warnings that could indicate runtime issues?

6. **Test Infrastructure Validity**:
   - Does jest.config.ts correctly configure ts-jest for the PCF TypeScript environment?
   - Do the ComponentFramework mocks (createMockDataset) accurately simulate the real DataSet API?
   - Does the FluentProvider helper (renderWithProviders) match the real runtime wrapping?
   - Are there tests that pass in Jest but would fail at runtime due to mock inaccuracy?
   - Are test fixtures representative of real Dataverse data shapes?

Write the findings document with the same structure as Task 1 but from the Implementability perspective. Focus on "will this actually work at runtime?" rather than "is the syntax correct?"
  </action>
  <verify>
    <automated>test -f .planning/phases/11-frontend-pcf-review/11-01-implementability-findings.md && grep -c "Deploy-Blocking\|Non-Blocking\|Validated" .planning/phases/11-frontend-pcf-review/11-01-implementability-findings.md</automated>
  </verify>
  <done>Implementability findings document exists with categorized issues (deploy-blocking/non-blocking), each describing whether a component/hook/config will work correctly at runtime in a real Canvas App PCF environment</done>
</task>

<task type="auto">
  <name>Task 3: Gaps Agent — identify missing error handling, untested paths, tech debt, and UX gaps</name>
  <files>.planning/phases/11-frontend-pcf-review/11-01-gaps-findings.md</files>
  <action>
Read every frontend/PCF source file, test file, and configuration file listed in the context section above. Act as the **Gaps Agent** — your mandate is to identify what is MISSING, what is ASSUMED but not handled, and what TECH DEBT remains that could affect deployment readiness.

**Checklist — work through each systematically:**

1. **v2.0 Tech Debt Assessment** (from PROJECT.md known tech debt list):
   For EACH item, classify as deploy-blocking or deferrable with rationale:
   - #7: Staleness polling (setInterval) lacks cleanup on unmount — Is this a memory leak risk?
   - #8: BriefingView test coverage thin on schedule logic — Does this create deployment risk?
   - #9: Command bar error states show raw error strings — Is this user-facing?
   - #10: No E2E flow coverage for send-email or set-reminder paths — Can we deploy without this?
   - #11: Confidence calibration thresholds are hardcoded — Is this a correctness issue or config issue?
   - #12: Sender profile 30-day window not configurable — Is this blocking for initial deployment?
   - #13: Daily briefing schedule stored in component state (lost on refresh) — Is this a data loss issue?
   Also identify any NEW tech debt not on this list.

2. **Missing Error Handling**:
   - For EACH component, verify: What happens when an unexpected error occurs during rendering? Is there an error boundary?
   - For useCardData: What happens when dataset.records throws? When a column name is missing? When type coercion fails?
   - For CommandBar: What happens when the orchestrator agent fails mid-execution? Timeout? Network error? Invalid response?
   - For BriefingCard: What happens when the briefing agent returns malformed data?
   - For ConfidenceCalibration: What happens when metric calculations divide by zero (no cards, no outcomes)?
   - For CardDetail: What happens when isSafeUrl throws on a completely invalid input?
   - For index.ts: What happens when React rendering throws? Is there a fallback?

3. **Missing Test Coverage**:
   - Compare source files to test files. Which components/hooks/utilities have NO test file?
   - For components WITH tests: which branches/paths are NOT covered? Specifically check:
     - ConfidenceCalibration.tsx — does it have test coverage? (324 lines, complex calculations)
     - BriefingCard.tsx — are schedule logic paths tested?
     - CommandBar.tsx — are error/timeout paths tested?
     - App.tsx — are all view states (gallery, detail, briefing, command, calibration) tested?
   - Are there integration-level concerns not covered by unit tests? (e.g., data flow from useCardData through App to CardDetail)
   - Is there test coverage for the PCF lifecycle (init/updateView/destroy in index.ts)?

4. **Missing UX Handling**:
   - Empty states: What does each component show when there is no data? (No cards, no briefing, no command history, no calibration data)
   - Loading states: Is there a loading indicator while data fetches? What about between page loads (paging)?
   - Error states: Are user-visible error messages friendly and actionable, or raw technical strings?
   - Accessibility: Are ARIA labels present? Is keyboard navigation supported? Can screen readers parse the dashboard?
   - Responsive behavior: While Canvas App PCF is desktop-only, does the control handle different canvas sizes?

5. **Missing Data Flow Connections**:
   - Is there a gap between what the Dataverse DataSet API provides and what the components expect?
   - Are there fields in the AssistantCard type that are never displayed in any component? (dead fields)
   - Are there fields displayed in components that could be undefined but are not guarded?
   - Is the output binding (PCF → Canvas App) fully specified for ALL actions that should communicate back? (e.g., card dismissal, email send, reminder creation)

6. **Configuration and Build Gaps**:
   - Are there missing ESLint rules that would catch common React mistakes?
   - Is the TypeScript strict mode configuration appropriate? (any use of `any` type?)
   - Are there missing npm scripts for common development tasks?
   - Is there a development/preview mode for testing the PCF control outside Canvas App?

Write the findings document with the same structure as Tasks 1 and 2 but from the Gaps perspective. For each gap, classify as:
- **Deploy-blocking gap**: Must be filled before deployment can succeed or users will encounter errors
- **Non-blocking gap**: Should be filled but deployment can proceed (degraded experience, not broken)
- **Known constraint**: PCF/Canvas App limitation with no workaround — document as accepted risk
  </action>
  <verify>
    <automated>test -f .planning/phases/11-frontend-pcf-review/11-01-gaps-findings.md && grep -c "Deploy-Blocking\|Non-Blocking\|Known Constraint\|Validated" .planning/phases/11-frontend-pcf-review/11-01-gaps-findings.md</automated>
  </verify>
  <done>Gaps findings document exists with categorized gaps (deploy-blocking/non-blocking/known constraint), identifying missing error handling, test coverage gaps, UX gaps, and tech debt classifications with evidence</done>
</task>

</tasks>

<verification>
After all three tasks complete:
1. Three separate findings documents exist in the phase directory
2. Each document has the required sections (Summary, Methodology, Deploy-Blocking Issues, Non-Blocking Issues, Validated)
3. Every PCF requirement is addressed:
   - PCF-01 (Component architecture): Covered by Correctness Task 1.4, Implementability Task 2.3-2.4, Gaps Task 3.4-3.5
   - PCF-02 (Tech debt categorization): Covered by Gaps Task 3.1 specifically (all 7 known items classified)
   - PCF-03 (Test coverage assessment): Covered by Gaps Task 3.3, Implementability Task 2.6
   - PCF-04 (Data flow verification): Covered by Correctness Task 1.2, Implementability Task 2.2, Gaps Task 3.5
   - PCF-05 (Error/UX handling): Covered by Gaps Task 3.2 and 3.4, Implementability Task 2.3
4. Issues are cross-referenced with specific file paths and code locations
5. Each issue has a severity classification (deploy-blocking vs. non-blocking)
</verification>

<success_criteria>
- Three independent findings documents produced, each covering all 5 PCF requirements from its unique perspective
- Every PCF source file (14 source files) has been reviewed by all three agents
- All 7 known v2.0 tech debt items are individually classified as deploy-blocking or deferrable
- Data flow from DataSet → useCardData → App → child components is fully traced
- Issues are specific and actionable (file paths, line numbers, code references)
- Deploy-blocking vs. non-blocking classification is applied to every issue
</success_criteria>

<output>
After completion, create `.planning/phases/11-frontend-pcf-review/11-01-SUMMARY.md`
</output>
