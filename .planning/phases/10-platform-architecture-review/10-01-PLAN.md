---
phase: 10-platform-architecture-review
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/10-platform-architecture-review/10-01-correctness-findings.md
  - .planning/phases/10-platform-architecture-review/10-01-implementability-findings.md
  - .planning/phases/10-platform-architecture-review/10-01-gaps-findings.md
autonomous: true
requirements:
  - PLAT-01
  - PLAT-02
  - PLAT-03
  - PLAT-04
  - PLAT-05

must_haves:
  truths:
    - "Every Dataverse column type and constraint in dataverse-table.json and senderprofile-table.json has been validated against real Power Platform type support"
    - "Every Power Automate flow step in agent-flows.md maps to a real connector action with correct expression syntax"
    - "Copilot Studio agent configs (prompts, topics, actions, entity refs) are complete with no orphaned references"
    - "Deployment scripts (provision-environment.ps1, deploy-solution.ps1, create-security-roles.ps1) use valid PAC CLI and Dataverse API operations"
    - "Platform limitations contradicting the design are identified with remediation paths"
  artifacts:
    - path: ".planning/phases/10-platform-architecture-review/10-01-correctness-findings.md"
      provides: "Correctness Agent findings — validates types, syntax, references are factually correct"
    - path: ".planning/phases/10-platform-architecture-review/10-01-implementability-findings.md"
      provides: "Implementability Agent findings — validates specs translate to buildable artifacts"
    - path: ".planning/phases/10-platform-architecture-review/10-01-gaps-findings.md"
      provides: "Gaps Agent findings — identifies missing pieces, undocumented assumptions, platform limitations"
  key_links:
    - from: "schemas/dataverse-table.json"
      to: "scripts/provision-environment.ps1"
      via: "Column definitions must match provisioning API calls"
      pattern: "cr_.*"
    - from: "schemas/output-schema.json"
      to: "docs/agent-flows.md"
      via: "Parse JSON schema in flows must match output schema fields"
      pattern: "Parse JSON"
    - from: "prompts/main-agent-system-prompt.md"
      to: "schemas/output-schema.json"
      via: "Agent prompt output format must match schema exactly"
      pattern: "trigger_type|triage_tier|draft_payload"
---

<objective>
Three independent AI Council agents review all platform-layer artifacts in parallel: Correctness (are types/syntax/references factually accurate?), Implementability (can each spec be built on a real Power Platform environment?), and Gaps (what is missing, undocumented, or contradicted by platform limitations?).

Purpose: Produce three independent review reports that cover every platform-layer artifact from different angles, ensuring no blind spots. Independent review prevents groupthink — each agent has its own mandate and checklist.

Output: Three findings documents, one per agent perspective, each with categorized issues (deploy-blocking vs. non-blocking), evidence, and remediation suggestions.
</objective>

<execution_context>
@/Users/admin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/admin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Platform-layer files under review (read ALL of these):
@enterprise-work-assistant/schemas/dataverse-table.json
@enterprise-work-assistant/schemas/senderprofile-table.json
@enterprise-work-assistant/schemas/output-schema.json
@enterprise-work-assistant/schemas/briefing-output-schema.json
@enterprise-work-assistant/docs/agent-flows.md
@enterprise-work-assistant/docs/deployment-guide.md
@enterprise-work-assistant/docs/canvas-app-setup.md
@enterprise-work-assistant/scripts/provision-environment.ps1
@enterprise-work-assistant/scripts/deploy-solution.ps1
@enterprise-work-assistant/scripts/create-security-roles.ps1
@enterprise-work-assistant/scripts/audit-table-naming.ps1
@enterprise-work-assistant/prompts/main-agent-system-prompt.md
@enterprise-work-assistant/prompts/humanizer-agent-prompt.md
@enterprise-work-assistant/prompts/daily-briefing-agent-prompt.md
@enterprise-work-assistant/prompts/orchestrator-agent-prompt.md
@enterprise-work-assistant/src/Solutions/Solution.cdsproj
</context>

<tasks>

<task type="auto">
  <name>Task 1: Correctness Agent — validate types, syntax, and references across platform artifacts</name>
  <files>.planning/phases/10-platform-architecture-review/10-01-correctness-findings.md</files>
  <action>
Read every platform-layer file listed in the context section above (all schemas, docs, scripts, prompts, and Solution.cdsproj). Act as the **Correctness Agent** — your mandate is to verify that every technical claim in these files is factually accurate against real Power Platform capabilities.

**Checklist — work through each systematically:**

1. **Dataverse Table Definitions** (dataverse-table.json, senderprofile-table.json):
   - Verify every `type` value is a real Dataverse column type (Text, Choice, Memo, Integer, DateTime, Boolean, Decimal, Lookup, etc.)
   - Verify `maxLength` values are within Dataverse limits (Text max 4000, Memo max 1048576)
   - Verify Choice option values start at 100000000 (Dataverse convention for custom choices)
   - Verify `ownershipType: "UserOwned"` is valid and supports row-level security
   - Verify relationship definitions (if any) use correct lookup syntax
   - Verify `alternateKeys` in senderprofile-table.json use supported key column types
   - Verify primary column (cr_itemsummary, cr_senderemail) types are valid for primary name attribute
   - Cross-check: every field referenced in output-schema.json has a corresponding Dataverse column

2. **Output Schemas** (output-schema.json, briefing-output-schema.json):
   - Verify JSON Schema draft-07 syntax is valid
   - Verify enum values match what the agent prompts actually produce
   - Verify nullable fields use correct `["type", "null"]` syntax vs `oneOf`
   - Verify the documented Power Automate Parse JSON limitation about oneOf/anyOf is accurate
   - Cross-check: every field the prompts promise to output exists in the schema

3. **Power Automate Flow Specs** (agent-flows.md):
   - Verify every connector action name exists (e.g., "When a new email arrives (V3)", "Add a new row", "Get my profile (V2)")
   - Verify expression syntax is correct Power Automate expression language (e.g., `@{outputs('...')?['body/...']}`)
   - Verify the Copilot Studio connector action name ("Execute Agent and wait") is the real action name
   - Verify Parse JSON schema compatibility claims (no oneOf/anyOf support) are accurate
   - Verify Choice column mapping uses integer values (not string labels) in "Add a new row" actions

4. **Deployment Scripts** (provision-environment.ps1, deploy-solution.ps1, create-security-roles.ps1):
   - Verify PAC CLI commands use real syntax (`pac env create`, `pac solution import`, etc.)
   - Verify Dataverse Web API endpoint patterns are correct (`/api/data/v9.2/EntityDefinitions`)
   - Verify Azure CLI token acquisition approach (`az account get-access-token --resource`) is valid
   - Verify PowerShell parameter validation attributes are syntactically correct
   - Verify the security role privilege depth values (1=User, 2=BU, 3=Parent-child BU, 4=Org) are correct

5. **Copilot Studio Agent Prompts** (main-agent-system-prompt.md, humanizer-agent-prompt.md, daily-briefing-agent-prompt.md, orchestrator-agent-prompt.md):
   - Verify runtime variable injection syntax ({{TRIGGER_TYPE}}, {{PAYLOAD}}, etc.) matches Copilot Studio conventions
   - Verify output format matches output-schema.json and briefing-output-schema.json exactly
   - Verify research tool references match what Copilot Studio supports (knowledge sources, actions)
   - Verify trigger types in prompts match the Choice options in dataverse-table.json

Write the findings document with this structure:
```
# Correctness Agent — Platform Architecture Findings

## Summary
[X issues found: N deploy-blocking, M non-blocking]

## Methodology
[What was checked and against what source of truth]

## Findings

### Deploy-Blocking Issues
[Issues that would prevent a successful deployment]
For each: ID, artifact, location, issue description, evidence, suggested fix

### Non-Blocking Issues
[Issues that are technically incorrect but would not prevent deployment]
For each: ID, artifact, location, issue description, evidence, suggested fix

### Validated (No Issues)
[Areas that passed correctness checks — brief list]
```

Be thorough. Cross-reference across files. If something looks correct but you cannot verify it against known platform docs, flag it as "unverified" with the specific claim that needs external validation.
  </action>
  <verify>
    <automated>test -f .planning/phases/10-platform-architecture-review/10-01-correctness-findings.md && grep -c "Deploy-Blocking\|Non-Blocking\|Validated" .planning/phases/10-platform-architecture-review/10-01-correctness-findings.md</automated>
  </verify>
  <done>Correctness findings document exists with categorized issues (deploy-blocking/non-blocking), each backed by specific evidence from platform artifact files and cross-references between schemas, prompts, scripts, and flow specs</done>
</task>

<task type="auto">
  <name>Task 2: Implementability Agent — validate specs translate to buildable Power Platform artifacts</name>
  <files>.planning/phases/10-platform-architecture-review/10-01-implementability-findings.md</files>
  <action>
Read every platform-layer file listed in the context section above. Act as the **Implementability Agent** — your mandate is to verify that every specification can be physically built and deployed on a real Power Platform environment using the current tooling and connectors.

**Checklist — work through each systematically:**

1. **Dataverse Table Buildability**:
   - Can the cr_assistantcard table be created via the provisioning script's API calls? Walk through the API sequence step by step.
   - Can the cr_senderprofile table be created? Does the script handle it or is it missing?
   - Are the column types compatible with Canvas App gallery/form controls?
   - Can the alternate key on cr_senderemail actually be created via the Web API?
   - Are there column count or table complexity limits that could be hit?

2. **Power Automate Flow Buildability**:
   - Can each flow be built using the standard designer (no premium connectors unless noted)?
   - Does the "Execute Agent and wait" action exist in the Microsoft Copilot Studio connector? What are its actual input/output parameters?
   - Can the Parse JSON action handle the simplified schema as documented?
   - Are there expression functions used that do not exist? (e.g., `coalesce()`, `if()`, `null` handling)
   - Can the "Add a new row" action handle all the column types (Choice, DateTime, Memo, etc.)?
   - Is the row ownership pattern (setting Owner via Azure AD Object ID) implementable?

3. **Copilot Studio Agent Buildability**:
   - Can the system prompt be set in Copilot Studio's current UI? (length limits, formatting support)
   - Can JSON output mode be enabled as described in the deployment guide?
   - Can research tool actions (SharePoint search, Graph API) be registered as described?
   - Can the agent handle the runtime variable injection pattern described in the prompts?
   - Can multiple agents (main, humanizer, briefing, orchestrator) coexist in one Copilot Studio environment?

4. **Deployment Script Buildability**:
   - Can `provision-environment.ps1` execute end-to-end on a fresh tenant? Trace the full execution path.
   - Can `deploy-solution.ps1` build and import a PCF solution? Verify the PAC CLI pack/import sequence.
   - Can `create-security-roles.ps1` create security roles via the Dataverse Web API? Verify the API endpoint and payload structure.
   - Are there missing steps between scripts? (e.g., publish customizations, assign roles to users)
   - Does the audit-table-naming.ps1 script serve a deployment purpose or is it dev-only?

5. **End-to-End Deployment Sequence**:
   - Walk through deployment-guide.md phases in order. Identify any step that references a UI location, button, or menu that may not exist or may have changed.
   - Identify any step that requires manual intervention not documented.
   - Identify any prerequisite (license, capacity, admin role) not documented.

Write the findings document with the same structure as Task 1 but from the Implementability perspective. Focus on "can this actually be built?" rather than "is the syntax correct?"
  </action>
  <verify>
    <automated>test -f .planning/phases/10-platform-architecture-review/10-01-implementability-findings.md && grep -c "Deploy-Blocking\|Non-Blocking\|Validated" .planning/phases/10-platform-architecture-review/10-01-implementability-findings.md</automated>
  </verify>
  <done>Implementability findings document exists with categorized issues (deploy-blocking/non-blocking), each describing whether a spec can physically be built on real Power Platform with current connectors and tooling</done>
</task>

<task type="auto">
  <name>Task 3: Gaps Agent — identify missing pieces, undocumented assumptions, and platform limitations</name>
  <files>.planning/phases/10-platform-architecture-review/10-01-gaps-findings.md</files>
  <action>
Read every platform-layer file listed in the context section above. Act as the **Gaps Agent** — your mandate is to identify what is MISSING, what is ASSUMED but not documented, and what PLATFORM LIMITATIONS contradict the design.

**Checklist — work through each systematically:**

1. **Missing Dataverse Definitions**:
   - Are there tables referenced in flows/prompts/code that do not have table definition JSON files? (e.g., conversation clustering data, reminder storage, command result storage)
   - Are there columns referenced in the prompts or flows that do not exist in the table definitions?
   - Are there indexes, views, or business rules that should exist but are not defined?
   - Is there a table for storing daily briefing schedules? (PROJECT.md notes schedule is in component state, lost on refresh — is a Dataverse table needed?)

2. **Missing Flow Definitions**:
   - The docs describe trigger flows for EMAIL, TEAMS_MESSAGE, CALENDAR_SCAN. Are flows for DAILY_BRIEFING, SELF_REMINDER, and COMMAND_RESULT trigger types fully specified?
   - Is the Card Outcome Tracker flow (referenced in senderprofile-table.json) fully specified?
   - Is the email send flow (fire-and-forget output binding) fully specified?
   - Are there flows for updating sender profiles that are referenced but not documented?

3. **Missing Copilot Studio Configuration**:
   - Are there topics, entities, or actions referenced in prompts that are not documented anywhere?
   - Is the multi-agent orchestration pattern (main + humanizer + briefing + orchestrator) fully specified? How are agents invoked from each other?
   - Are knowledge source configurations documented? (which SharePoint sites, Graph scopes)
   - Is the JSON output mode configuration documented step-by-step?

4. **Undocumented Assumptions**:
   - What licenses are required? (Copilot Studio, Power Automate Premium, Dataverse)
   - What admin roles are required for each deployment step?
   - What connector permissions/consent is needed?
   - What are the API rate limits that could affect flow execution?
   - What happens if the agent times out? Is there a timeout handling strategy?
   - What is the assumed data volume? (cards per user per day, sender profiles per user)

5. **Platform Limitations That Contradict Design**:
   - Copilot Studio system prompt length limits vs. the length of the actual prompts
   - Power Automate expression limitations (string length, nesting depth, loop limits)
   - Dataverse API batch size limits for the provisioning script
   - PCF control limitations in Canvas Apps (event handling, data binding)
   - Canvas App delegation limits for Dataverse queries (500/2000 row default)
   - Copilot Studio connector response size limits
   - Any deprecated APIs or connectors referenced

Write the findings document with the same structure as Tasks 1 and 2 but from the Gaps perspective. For each gap, classify as:
- **Deploy-blocking gap**: Must be filled before deployment can succeed
- **Non-blocking gap**: Should be filled but deployment can proceed without it
- **Known constraint**: Platform limitation with no workaround — document as accepted risk
  </action>
  <verify>
    <automated>test -f .planning/phases/10-platform-architecture-review/10-01-gaps-findings.md && grep -c "Deploy-Blocking\|Non-Blocking\|Known Constraint\|Validated" .planning/phases/10-platform-architecture-review/10-01-gaps-findings.md</automated>
  </verify>
  <done>Gaps findings document exists with categorized gaps (deploy-blocking/non-blocking/known constraint), identifying missing definitions, undocumented assumptions, and platform limitations with evidence</done>
</task>

</tasks>

<verification>
After all three tasks complete:
1. Three separate findings documents exist in the phase directory
2. Each document has the required sections (Summary, Methodology, Deploy-Blocking Issues, Non-Blocking Issues, Validated)
3. Every PLAT requirement is addressed:
   - PLAT-01 (Dataverse validity): Covered by Correctness Task 1.1-1.2, Implementability Task 2.1, Gaps Task 3.1
   - PLAT-02 (Flow buildability): Covered by Correctness Task 1.3, Implementability Task 2.2, Gaps Task 3.2
   - PLAT-03 (Copilot Studio validity): Covered by Correctness Task 1.5, Implementability Task 2.3, Gaps Task 3.3
   - PLAT-04 (Deployment scripts): Covered by Correctness Task 1.4, Implementability Task 2.4, Gaps Task 3.4-3.5
   - PLAT-05 (Platform limitations): Covered by Gaps Task 3.5 specifically, plus constraint flags in all three reports
4. Issues are cross-referenced with specific file paths and line numbers where possible
5. Each issue has a severity classification (deploy-blocking vs. non-blocking)
</verification>

<success_criteria>
- Three independent findings documents produced, each covering all 5 PLAT requirements from its unique perspective
- Every platform-layer file has been reviewed by all three agents
- Issues are specific and actionable (not vague observations)
- Deploy-blocking vs. non-blocking classification is applied to every issue
- Cross-references between files are checked (schema-to-prompt, schema-to-flow, schema-to-script)
</success_criteria>

<output>
After completion, create `.planning/phases/10-platform-architecture-review/10-01-SUMMARY.md`
</output>
